{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMO效果测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T10:55:28.468055Z",
     "start_time": "2019-08-22T10:55:24.352106Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from bilm import TokenBatcher, BidirectionalLanguageModel, weight_layers\n",
    "\n",
    "\n",
    "class ELMO(object):\n",
    "    def __init__(self, character=False, model_path=\"./\"):\n",
    "        vocab_file = os.path.join(\"./\", 'seg_words.txt')\n",
    "        options_file = os.path.join(model_path, 'options.json')\n",
    "        weight_file = os.path.join(model_path,'weights.hdf5')\n",
    "        token_embedding_file = os.path.join(model_path, 'vocab_embedding.hdf5')\n",
    "        self.batcher = TokenBatcher(vocab_file)\n",
    "        self.bilm = BidirectionalLanguageModel(options_file, weight_file, use_character_inputs=character, \n",
    "                                               embedding_weight_file=token_embedding_file)\n",
    "        self.context_token_ids = tf.placeholder('int32', shape=(None, None))\n",
    "        self.context_embeddings_op = self.bilm(self.context_token_ids)\n",
    "        self.elmo_context_input = weight_layers('input', self.context_embeddings_op, l2_coef=0.0)\n",
    "        self.elmo_context_output = weight_layers('output', self.context_embeddings_op, l2_coef=0.0)\n",
    "        self.ses = tf.Session()\n",
    "        self.ses.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def to_vector(self, text_list):\n",
    "        context_ids = self.batcher.batch_sentences(text_list)\n",
    "        return self.ses.run(self.elmo_context_input['weighted_op'], feed_dict={self.context_token_ids: context_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T06:56:09.370816Z",
     "start_time": "2019-08-20T06:56:09.341415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020879745483398438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 3, 1024)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "a = model.to_vector([list('这是啥'), ])\n",
    "print(time.time()-t0)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T03:00:36.715071Z",
     "start_time": "2019-08-19T03:00:36.709562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.00932189, -0.59012043, -0.17174587, ..., -0.39263338,\n",
       "         -0.6241608 ,  0.1475308 ],\n",
       "        [-0.04369204,  0.07538365, -0.5509269 , ...,  0.35087165,\n",
       "         -0.29011977,  0.03631983],\n",
       "        [ 0.44123632, -0.06315079, -0.29441255, ...,  0.36990762,\n",
       "         -0.24927339, -0.06850408]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum(axis=1).shape\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T10:55:37.530146Z",
     "start_time": "2019-08-22T10:55:36.495500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train a maxlen 30\n",
      "train b maxlen 30\n",
      "dev a maxlen 30\n",
      "dev b maxlen 30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {\"train\": None, \"dev\": None}\n",
    "def format_str(s):\n",
    "    if len(s) < 30:\n",
    "        return list(s)+['<UNK>']*(30-len(s))\n",
    "    else:\n",
    "        return list(s[0:30])\n",
    "def format_label(yy):\n",
    "    origin = [0, 0]\n",
    "    origin[yy] += 1\n",
    "    return origin\n",
    "for k in data:\n",
    "    data[k] = pd.read_csv(\"../data/za_data/kd_{}.csv\".format(k), header=None, sep=\"\\t\", names=(\"a\", \"b\", \"y\"))\n",
    "    data[k][\"y\"] = data[k][\"y\"].apply(format_label)\n",
    "    for c in [\"a\", \"b\"]:\n",
    "        data[k][c] = data[k][c].apply(format_str)\n",
    "        print(\"{} {} maxlen {}\".format(k, c, data[k][c].apply(len).max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T10:57:01.849622Z",
     "start_time": "2019-08-22T10:57:01.774900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[嗯, ，, 然, 后, 要, 交, 钱, 是, 吗, ，, &lt;UNK&gt;, &lt;UNK&gt;, &lt;...</td>\n",
       "      <td>[我, 那, 个, 珍, 爱, 网, ，, 那, 是, 别, 人, 瞎, 给, 我, 整, ...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[嗯, 你, 这, 是, 你, 到, 哪, 边, 的, &lt;UNK&gt;, &lt;UNK&gt;, &lt;UNK...</td>\n",
       "      <td>[我, 我, 我, 这, 好, 像, 是, 我, 朋, 友, 的, 帮, 我, 讲, 的, ...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[我, 想, 就, 是, 网, 上, 太, 假, 了, &lt;UNK&gt;, &lt;UNK&gt;, &lt;UNK...</td>\n",
       "      <td>[你, ，, 你, 能, 看, 出, 我, 需, 要, 什, 么, 样, 的, 对, 象, ...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[您, 来, 电, 话, 的, 话, 收, 费, 吗, ，, &lt;UNK&gt;, &lt;UNK&gt;, &lt;...</td>\n",
       "      <td>[你, 啊, ，, 还, 是, 单, 身, 了, 你, ，, 你, 在, 这, 里, 什, ...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[那, 你, 们, 这, 边, 要, 收, 费, 吗, ，, 这, 个, 收, 费, 怎, ...</td>\n",
       "      <td>[嗯, 你, 是, 哪, 个, 哪, 个, 平, 台, &lt;UNK&gt;, &lt;UNK&gt;, &lt;UNK...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   a  \\\n",
       "0  [嗯, ，, 然, 后, 要, 交, 钱, 是, 吗, ，, <UNK>, <UNK>, <...   \n",
       "1  [嗯, 你, 这, 是, 你, 到, 哪, 边, 的, <UNK>, <UNK>, <UNK...   \n",
       "2  [我, 想, 就, 是, 网, 上, 太, 假, 了, <UNK>, <UNK>, <UNK...   \n",
       "3  [您, 来, 电, 话, 的, 话, 收, 费, 吗, ，, <UNK>, <UNK>, <...   \n",
       "4  [那, 你, 们, 这, 边, 要, 收, 费, 吗, ，, 这, 个, 收, 费, 怎, ...   \n",
       "\n",
       "                                                   b       y  \n",
       "0  [我, 那, 个, 珍, 爱, 网, ，, 那, 是, 别, 人, 瞎, 给, 我, 整, ...  [1, 0]  \n",
       "1  [我, 我, 我, 这, 好, 像, 是, 我, 朋, 友, 的, 帮, 我, 讲, 的, ...  [1, 0]  \n",
       "2  [你, ，, 你, 能, 看, 出, 我, 需, 要, 什, 么, 样, 的, 对, 象, ...  [1, 0]  \n",
       "3  [你, 啊, ，, 还, 是, 单, 身, 了, 你, ，, 你, 在, 这, 里, 什, ...  [1, 0]  \n",
       "4  [嗯, 你, 是, 哪, 个, 哪, 个, 平, 台, <UNK>, <UNK>, <UNK...  [1, 0]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"dev\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T10:57:04.861303Z",
     "start_time": "2019-08-22T10:57:04.835885Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_iter(df_gen, batch_size, shuffle=True):\n",
    "    obs = len(df_gen)\n",
    "    if shuffle:\n",
    "        data_gen = df_gen.sample(frac=1).reset_index(drop=True)\n",
    "    else:\n",
    "        data_gen = df_gen.copy()\n",
    "    batch_num = int(obs/batch_size)\n",
    "    for j in range(batch_num):\n",
    "        yield (data_gen[\"a\"].iloc[j*batch_size:min(obs, j*batch_size+batch_size)].tolist(),\n",
    "        data_gen[\"b\"].iloc[j*batch_size:min(obs, j*batch_size+batch_size)].tolist(),\n",
    "        data_gen[\"y\"].iloc[j*batch_size:min(obs, j*batch_size+batch_size)].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T10:57:20.215934Z",
     "start_time": "2019-08-22T10:57:20.187695Z"
    }
   },
   "outputs": [],
   "source": [
    "class FCLayer(object):\n",
    "    def __init__(self, num_in, num_out):\n",
    "        self.num_in = num_in\n",
    "        self.num_out = num_out\n",
    "        self.weight = tf.Variable(tf.random_normal([num_in, num_out]))\n",
    "        self.bias = tf.Variable(tf.random_normal([num_out]))\n",
    "\n",
    "    def ops(self, input_x):\n",
    "        out_without_bias = tf.matmul(input_x, self.weight)\n",
    "        output = tf.nn.bias_add(out_without_bias, self.bias)\n",
    "        return output\n",
    "    \n",
    "class MlpMatch(object):\n",
    "    def __init__(self):\n",
    "        self.n_class = 2\n",
    "        self.emb_size = 1024\n",
    "        self.hidden_size = 128\n",
    "        self.bow_layer = FCLayer(self.emb_size, self.hidden_size)\n",
    "        self.fc_layer = FCLayer(2 * self.hidden_size, self.n_class)\n",
    "     \n",
    "    def predict(self, left_slots, right_slots):\n",
    "        left, right = left_slots, right_slots\n",
    "        left_bow = self.bow_layer.ops(tf.nn.softsign(tf.reduce_sum(left, axis=1)))\n",
    "        right_bow = self.bow_layer.ops(tf.nn.softsign(tf.reduce_sum(right, axis=1)))\n",
    "        concat = tf.concat([left_bow, right_bow], -1)\n",
    "        pred = self.fc_layer.ops(concat)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T11:23:52.392252Z",
     "start_time": "2019-08-22T10:57:41.020030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING SKIP CONNECTIONS\n",
      "WARNING:tensorflow:From /data/home/fanzhengfeng/.local/lib/python3.6/site-packages/bilm-0.1.post5-py3.6.egg/bilm/elmo.py:89: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "WARNING:tensorflow:From <ipython-input-8-b499b9b2504d>:15: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "     train loss: 0.367286\n",
      "     epoch 1 accuracy on validation data : 0.589648\n",
      "     train loss: 0.255453\n",
      "     epoch 2 accuracy on validation data : 0.616016\n",
      "     train loss: 0.126862\n",
      "     epoch 3 accuracy on validation data : 0.623438\n",
      "     train loss: 0.158700\n",
      "     epoch 4 accuracy on validation data : 0.623047\n",
      "     train loss: 0.125178\n",
      "     epoch 5 accuracy on validation data : 0.624609\n",
      "     train loss: 0.162531\n",
      "     epoch 6 accuracy on validation data : 0.623438\n",
      "     train loss: 0.141452\n",
      "     epoch 7 accuracy on validation data : 0.630469\n",
      "     train loss: 0.092454\n",
      "     epoch 8 accuracy on validation data : 0.632422\n",
      "     train loss: 0.128707\n",
      "     epoch 9 accuracy on validation data : 0.627148\n",
      "     train loss: 0.058363\n",
      "     epoch 10 accuracy on validation data : 0.638867\n",
      "     train loss: 0.093374\n",
      "     epoch 11 accuracy on validation data : 0.640039\n",
      "     train loss: 0.064890\n",
      "     epoch 12 accuracy on validation data : 0.628711\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model = ELMO(character=False, model_path='/data/zhangminchao/new_elmo')\n",
    "tf.reset_default_graph()\n",
    "net = MlpMatch()\n",
    "\n",
    "test_l = tf.placeholder(tf.float32, [None, 30, 1024], name=\"input_left\")\n",
    "test_r = tf.placeholder(tf.float32, [None, 30, 1024], name=\"input_right\")\n",
    "test_y = tf.placeholder(tf.float32, [None, 2], name=\"input_label\")\n",
    "\n",
    "pred = net.predict(test_l, test_r)\n",
    "pred_prob = tf.nn.softmax(pred, -1)\n",
    "pred_index = tf.argmax(pred_prob, 1)\n",
    "correct_pred = tf.equal(pred_index, tf.argmax(test_y, 1))\n",
    "acc = tf.reduce_mean(tf.cast(correct_pred, \"float\"))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=test_y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(12):\n",
    "        for input_l, input_r, input_y in batch_iter(data[\"train\"], batch_size=128, shuffle=True):\n",
    "            cost, _ = sess.run([loss, optimizer], feed_dict={test_l: model.to_vector(input_l).astype(np.float32), \n",
    "                                                             test_r:  model.to_vector(input_r).astype(np.float32), \n",
    "                                                             test_y: np.array(input_y).astype(np.float32)})\n",
    "        accuracy = []\n",
    "        for val_l, val_r, val_y in batch_iter(data[\"dev\"], batch_size=128, shuffle=False):\n",
    "            accuracy.append(sess.run(acc, feed_dict={test_l:  model.to_vector(val_l).astype(np.float32), \n",
    "                                                     test_r: model.to_vector(val_r).astype(np.float32), \n",
    "                                                     test_y: np.array(val_y).astype(np.float32)}))\n",
    "        print(\" \"*4, \"train loss: %f\" % (cost/len(input_l)))\n",
    "        print(\" \"*4, \"epoch %d accuracy on validation data : %f\" % (e+1, np.mean(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "train loss: 0.498080\n",
    "     epoch 1 accuracy on validation data : 0.510352\n",
    "     train loss: 0.464507\n",
    "     epoch 2 accuracy on validation data : 0.516797\n",
    "     train loss: 0.448489\n",
    "     epoch 3 accuracy on validation data : 0.519336\n",
    "     train loss: 0.433034\n",
    "     epoch 4 accuracy on validation data : 0.516406\n",
    "     train loss: 0.394427\n",
    "     epoch 5 accuracy on validation data : 0.517383\n",
    "     train loss: 0.390801\n",
    "     epoch 6 accuracy on validation data : 0.520117\n",
    "     train loss: 0.261450\n",
    "     epoch 7 accuracy on validation data : 0.521680\n",
    "     train loss: 0.328833\n",
    "     epoch 8 accuracy on validation data : 0.523633\n",
    "     train loss: 0.306248\n",
    "     epoch 9 accuracy on validation data : 0.529492\n",
    "     train loss: 0.310917\n",
    "     epoch 10 accuracy on validation data : 0.534180\n",
    "     train loss: 0.341402\n",
    "     epoch 11 accuracy on validation data : 0.540625\n",
    "     train loss: 0.291676\n",
    "     epoch 12 accuracy on validation data : 0.541602\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T14:17:19.751391Z",
     "start_time": "2019-08-22T13:52:12.573659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING SKIP CONNECTIONS\n",
      "     train loss: 0.285930\n",
      "     epoch 1 accuracy on validation data : 0.568945\n",
      "     train loss: 0.306973\n",
      "     epoch 2 accuracy on validation data : 0.583789\n",
      "     train loss: 0.153048\n",
      "     epoch 3 accuracy on validation data : 0.600977\n",
      "     train loss: 0.158269\n",
      "     epoch 4 accuracy on validation data : 0.604883\n",
      "     train loss: 0.150963\n",
      "     epoch 5 accuracy on validation data : 0.611523\n",
      "     train loss: 0.148603\n",
      "     epoch 6 accuracy on validation data : 0.569727\n",
      "     train loss: 0.098603\n",
      "     epoch 7 accuracy on validation data : 0.623633\n",
      "     train loss: 0.103967\n",
      "     epoch 8 accuracy on validation data : 0.615430\n",
      "     train loss: 0.085192\n",
      "     epoch 9 accuracy on validation data : 0.628711\n",
      "     train loss: 0.082350\n",
      "     epoch 10 accuracy on validation data : 0.633398\n",
      "     train loss: 0.114665\n",
      "     epoch 11 accuracy on validation data : 0.637109\n",
      "     train loss: 0.062814\n",
      "     epoch 12 accuracy on validation data : 0.635156\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model = ELMO(character=False, model_path='./')\n",
    "tf.reset_default_graph()\n",
    "net = MlpMatch()\n",
    "test_l = tf.placeholder(tf.float32, [None, 30, 1024], name=\"input_left\")\n",
    "test_r = tf.placeholder(tf.float32, [None, 30, 1024], name=\"input_right\")\n",
    "test_y = tf.placeholder(tf.float32, [None, 2], name=\"input_label\")\n",
    "\n",
    "pred = net.predict(test_l, test_r)\n",
    "pred_prob = tf.nn.softmax(pred, -1)\n",
    "pred_index = tf.argmax(pred_prob, 1)\n",
    "correct_pred = tf.equal(pred_index, tf.argmax(test_y, 1))\n",
    "acc = tf.reduce_mean(tf.cast(correct_pred, \"float\"))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=test_y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(12):\n",
    "        for input_l, input_r, input_y in batch_iter(data[\"train\"], batch_size=128, shuffle=True):\n",
    "            cost, _ = sess.run([loss, optimizer], feed_dict={test_l: model.to_vector(input_l).astype(np.float32), \n",
    "                                                             test_r:  model.to_vector(input_r).astype(np.float32), \n",
    "                                                             test_y: np.array(input_y).astype(np.float32)})\n",
    "        accuracy = []\n",
    "        for val_l, val_r, val_y in batch_iter(data[\"dev\"], batch_size=128, shuffle=False):\n",
    "            accuracy.append(sess.run(acc, feed_dict={test_l:  model.to_vector(val_l).astype(np.float32), \n",
    "                                                     test_r: model.to_vector(val_r).astype(np.float32), \n",
    "                                                     test_y: np.array(val_y).astype(np.float32)}))\n",
    "        print(\" \"*4, \"train loss: %f\" % (cost/len(input_l)))\n",
    "        print(\" \"*4, \"epoch %d accuracy on validation data : %f\" % (e+1, np.mean(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T11:23:52.486111Z",
     "start_time": "2019-08-22T11:23:52.479453Z"
    }
   },
   "source": [
    "```\n",
    "train loss: 0.783122\n",
    "     epoch 1 accuracy on validation data : 0.526953\n",
    "     train loss: 0.612946\n",
    "     epoch 2 accuracy on validation data : 0.532227\n",
    "     train loss: 0.706031\n",
    "     epoch 3 accuracy on validation data : 0.531641\n",
    "     train loss: 0.736148\n",
    "     epoch 4 accuracy on validation data : 0.537305\n",
    "     train loss: 0.615426\n",
    "     epoch 5 accuracy on validation data : 0.540039\n",
    "     train loss: 0.522318\n",
    "     epoch 6 accuracy on validation data : 0.541016\n",
    "     train loss: 0.454612\n",
    "     epoch 7 accuracy on validation data : 0.543164\n",
    "     train loss: 0.543438\n",
    "     epoch 8 accuracy on validation data : 0.545703\n",
    "     train loss: 0.563003\n",
    "     epoch 9 accuracy on validation data : 0.546289\n",
    "     train loss: 0.514223\n",
    "     epoch 10 accuracy on validation data : 0.551562\n",
    "     train loss: 0.615924\n",
    "     epoch 11 accuracy on validation data : 0.552344\n",
    "     train loss: 0.498719\n",
    "     epoch 12 accuracy on validation data : 0.555078\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T07:56:12.082489Z",
     "start_time": "2019-08-15T07:56:12.025670Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['这', '是', '测试', '.'], ['好的', '.']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_context = ['这 是 测试 .', '好的 .']\n",
    "tokenized_context = [sentence.split() for sentence in raw_context]\n",
    "tokenized_question = [['这', '是', '什么'],]\n",
    "tokenized_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T03:39:17.592814Z",
     "start_time": "2019-08-15T03:38:44.570814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING SKIP CONNECTIONS\n",
      "USING SKIP CONNECTIONS\n",
      "[[[-0.0494836   0.15072705  0.02371956 ...  0.01728119  0.15322769\n",
      "   -0.2924335 ]\n",
      "  [-0.03865705  0.0443635   0.194944   ...  0.20702595 -0.17606495\n",
      "   -0.15220994]\n",
      "  [-0.02532118  0.07337871  0.08422874 ...  0.1530999   0.12955403\n",
      "   -0.05277611]\n",
      "  [-0.03031924  0.03041445  0.11669184 ...  0.06387962  0.06061445\n",
      "   -0.00702057]]\n",
      "\n",
      " [[ 0.37764782 -0.00799875  0.166332   ...  0.1530999   0.12955403\n",
      "   -0.05277611]\n",
      "  [ 0.21324293  0.02273425  0.21007645 ...  0.06387962  0.06061445\n",
      "   -0.00702057]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]] [[[-0.04948363  0.15072708  0.02371956 ...  0.02699655  0.21408397\n",
      "   -0.1234289 ]\n",
      "  [-0.03865702  0.04436348  0.194944   ...  0.21258295 -0.12991495\n",
      "    0.07850938]\n",
      "  [-0.02532118  0.07337872  0.08422876 ...  0.06387964  0.06061444\n",
      "   -0.00702058]]]\n"
     ]
    }
   ],
   "source": [
    "context_token_ids = tf.placeholder('int32', shape=(None, None))\n",
    "question_token_ids = tf.placeholder('int32', shape=(None, None))\n",
    "\n",
    "context_embeddings_op = bilm(context_token_ids)\n",
    "# question_embeddgins_op = bilm(question_token_ids)\n",
    "\n",
    "elmo_context_input = weight_layers('input', context_embeddings_op, l2_coef=0.0)\n",
    "elmo_context_output = weight_layers('output', context_embeddings_op, l2_coef=0.0)\n",
    "\n",
    "# with tf.variable_scope('', reuse=True):\n",
    "#     elmo_question_input = weight_layers('input', question_embeddgins_op, l2_coef=0.0)\n",
    "# with tf.variable_scope('', reuse=True):\n",
    "#     elmo_question_output = weight_layers('output', question_embeddgins_op, l2_coef=0.0)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    context_ids = batcher.batch_sentences(tokenized_context)\n",
    "#     question_ids = batcher.batch_sentences(tokenized_question)\n",
    "# elmo_question_input_\n",
    "    elmo_context_input_ = sess.run(\n",
    "        [\n",
    "            elmo_context_input['weighted_op']\n",
    "#             elmo_question_input['weighted_op']\n",
    "        ],\n",
    "        feed_dict={context_token_ids: context_ids, question_token_ids: question_ids})\n",
    "\n",
    "# print(elmo_context_input_, elmo_question_input_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T03:46:36.357960Z",
     "start_time": "2019-08-15T03:46:36.351508Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 1024)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo_context_input_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T03:47:06.854230Z",
     "start_time": "2019-08-15T03:47:06.847585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 1024)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo_question_input_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T06:59:16.149561Z",
     "start_time": "2019-08-15T06:58:59.042093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING SKIP CONNECTIONS\n",
      "[array([[[-0.0494836 ,  0.15072705,  0.02371956, ...,  0.01728119,\n",
      "          0.15322769, -0.2924335 ],\n",
      "        [-0.03865705,  0.0443635 ,  0.194944  , ...,  0.20702595,\n",
      "         -0.17606495, -0.15220994],\n",
      "        [-0.02532118,  0.07337871,  0.08422874, ...,  0.1530999 ,\n",
      "          0.12955403, -0.05277611],\n",
      "        [-0.03031924,  0.03041445,  0.11669184, ...,  0.06387962,\n",
      "          0.06061445, -0.00702057]],\n",
      "\n",
      "       [[ 0.37764782, -0.00799875,  0.166332  , ...,  0.1530999 ,\n",
      "          0.12955403, -0.05277611],\n",
      "        [ 0.21324293,  0.02273425,  0.21007645, ...,  0.06387962,\n",
      "          0.06061445, -0.00702057],\n",
      "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "          0.        ,  0.        ]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "print(elmo_context_input_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
