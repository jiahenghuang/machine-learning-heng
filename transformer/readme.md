这个链接讲的挺好：https://www.julyedu.com/question/big/kp_id/30/ques_id/2912

Layer Normalization
归一化的本质都是将数据转化为均值为0，方差为1的数据。这样可以减小数据的偏差，规避训练过程中梯度消失或爆炸的情况。我们在训练网络中比较常见的归一化方法是Batch Normalization，即在每一层输出的每一批数据上进行归一化。而Layer Normalization与BN稍有不同，即在每一层输出的每一个样本上进行归一化。

5、Residual connection

残差连接其实在很多网络机构中都有用到。原理很简单，假设一个输入向量x，经过一个网络结构，得到输出向量f(x)，加上残差连接，相当于在输出向量中加入输入向量，即输出结构变为f(x)+x，这样做的好处是在对x求偏导时，加入一项常数项1，避免了梯度消失的问题。

6、Layer Normalization

归一化的本质都是将数据转化为均值为0，方差为1的数据。这样可以减小数据的偏差，规避训练过程中梯度消失或爆炸的情况。我们在训练网络中比较常见的归一化方法是Batch Normalization，即在每一层输出的每一批数据上进行归一化。而Layer Normalization与BN稍有不同，即在每一层输出的每一个样本上进行归一化。

7、Mask

mask的思想非常简单：就是对输入序列中没某些值进行掩盖，使其不起作用。在论文中，做multi-head attention的地方用到了padding mask，在decode输入数据中用到了sequence mask。

（1）、padding mask

在我们输入的数据中，因为每句话的长度不同，所以要对较短的数据进行填充补齐长度。而这些填充值并没有什么作用，为了减少填充数据对attention计算的影响，采用padding mask的机制，即在填充物的位置上加上一个趋紧于负无穷的负数，这样经过softmax计算后这些位置的概率会趋近于0

（2）、sequence mask

在上文中我们提到，预测t时刻的输出值yt，应该使用全部的输入序列X，和t时刻之前的输出序列(y1,y2,……,yt-1)进行预测。所以在训练时，应该将t-1时刻之后的信息全部隐藏掉。所以需要用到sequence mask。

实现也很简单，就是用一个上三角矩阵，上三角值均为1，下三角值均为0，对角线值为0，与输入序列相乘，就达到了目的。

以上就是Transformer框架的全部知识点，BERT模型也是在此基础上发展而来。

