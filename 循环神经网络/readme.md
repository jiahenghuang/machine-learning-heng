这篇讲循环神经网络的讲的太好了。我不需要总结了。
里面不懂的地方是：向量对向量求导数的时候，为Jacobian矩阵，非对角元素为0，这一点请教了非常多的人，没人能解释，暂时放弃这个点。

面试经常问的一个问题：梯度爆炸、梯度消失产生的原因是什么？

1. 损失函数需要对w求导数，损失函数是net的函数，对w求导就首先对net求导，然后net需要往前逐步求导，损失函数对net的导数记为delta，delta往前传递的时候需要乘以net对前面net的导数，
这个导数可以分解为两个值，一个net对s求导乘以s对前一个net求导，向量对向量求导数，前一个是w矩阵，后一个是对角阵。当反向传播的时候会有多个w矩阵连乘，同时多个对角矩阵也会乘积，多个
对角矩阵相乘就意味着 x^n。如果x大于1则梯度爆炸，如果x小于1则梯度消失。




