发现pdf文件少了很多公式，附上网址： https://zybuluo.com/hanbingtao/note/541458
这篇讲循环神经网络的讲的太好了。我不需要总结了。
里面不懂的地方是：向量对向量求导数的时候，为Jacobian矩阵，非对角元素为0，这一点请教了非常多的人，没人能解释，暂时放弃这个点。

面试经常问的一个问题：梯度爆炸、梯度消失产生的原因是什么？

1. 损失函数需要对w求导数，损失函数是net的函数，对w求导就首先对net求导，然后net需要往前逐步求导，损失函数对net的导数记为delta，delta往前传递的时候需要乘以net对前面net的导数，
这个导数可以分解为两个值，一个net对s求导乘以s对前一个net求导，向量对向量求导数，前一个是w矩阵，后一个是对角阵。当反向传播的时候会有多个w矩阵连乘，同时多个对角矩阵也会乘积，多个
对角矩阵相乘就意味着 x^n。如果x大于1则梯度爆炸，如果x小于1则梯度消失。

2. 如何解决梯度消失问题？

1）合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。  如何初始化权重有很多的研究方法。没研究过
2）使用relu代替sigmoid和tanh作为激活函数。
3）使用其他结构的RNNs，比如长短时记忆网络（LTSM）和gru

3. relu看起来长得是线性函数的样子，这样不是会丢失深度学习的优势吗（对非线性对学习）

在真正一个模型中，relu仅仅是几层的激活函数，仍然会配合非线性的激活函数，如最后一曾基本都是softmax吧。然而，我们往往以为非线性的特征比较多，然而真正情况下，非线性特征并没那么多的卵用，那只是个错觉。

4. 从rnn角度解释，为什么relu可以解决梯度消失？
   s=f(net),需要对net激活，s对net的导数，如果是rnn的话，这个导数为1，或者为0. 损失函数对最后一个net对梯度往前面传播的时候不会降低或者放弃传播。