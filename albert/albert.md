# albert
## 低秩分解
1. 先把第一次的映射放到很低的维度，然后在映射为我们需要的维度。这样做有两个好处：

词的Context independent表示与Context dependent表示之间解锁，可以自由的对Context dependent表示进行加高，也就是网络变宽。
One-hot向量到第一次映射的参数非常多，可以把这块参数变的非常小。

2.矩阵分解本质上就是一个低秩分解的操作，其通过对Embedding 部分降维来达到降低参数的作用。在最初的BERT中，以Base为例，Embedding层的维度与隐层的维度一样都是768，但是我们知道，对于词的分布式表示，往往并不需要这么高的维度，比如在Word2Vec时代最多采用50或300这样的维度。那么一个很简单的思想就是，通过将Embedding部分分解来达到降低参数量的作

我们以 BERT-Base 为例，Base中的Hidden size 为768， 词表大小为3w，此时的参数量为：768 * 3w = 23040000。如果将 Embedding 的维度改为 128，那么此时Embedding层的参数量为：128 * 3w + 128 * 768 = 3938304。二者的差为19101696，大约为19M。我们看到，其实Embedding参数量从原来的23M变为了现在的4M，似乎变化特别大，然而当我们放到全局来看的话，BERT-Base的参数量在110M，降低19M也不能产生什么革命性的变化。因此，可以说Embedding层的因式分解其实并不是降低参数量的主要手段。

## 跨层参数共享
ALBERT提出的另一个减少参数量的方法就是层之间的参数共享，即多个层使用相同的参数。参数共享有三种方式：只共享feed-forward network的参数、只共享attention的参数、共享全部参数。ALBERT默认是共享全部参数的。可以理解为此时的权值共享并不是说12层transformer_encoder的值是一样的，只是占用了同一变量，所以模型的计算量并没有少，只是参数个数变成了原来的12分之一。


## 预训练策略 SOP 替代 NSP
### 就是a的下文本来是b：
### 正样本是a的下文是b
### 负样本是b的下文是a